{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.ImageFolder(\"extracted_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for tup in data.imgs:\n",
    "        path = tup[0]\n",
    "        label = tup[1]\n",
    "        image = Image.open(path)\n",
    "        converter = torchvision.transforms.ToTensor()\n",
    "        x.append(converter.__call__(image).numpy())\n",
    "        y.append(label)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = create_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWrittenDataset(Dataset):\n",
    "    \n",
    "    # TODO:\n",
    "    # Define constructor for AnimalDataset class\n",
    "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
    "    def __init__(self, x,y):\n",
    "        super(HandWrittenDataset, self).__init__()\n",
    "        self.data = x\n",
    "        self.truth = y\n",
    "    '''This function should return sample count in the dataset'''\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
    "    def __getitem__(self, index):\n",
    "        _x = self.data[index]\n",
    "        _y = self.truth[index]\n",
    "        return _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method HandWrittenDataset.__len__ of <__main__.HandWrittenDataset object at 0x108fb0b00>>\n"
     ]
    }
   ],
   "source": [
    "all_data = HandWrittenDataset(x,y)\n",
    "x.shape[0]\n",
    "print(all_data.__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset = torch.utils.data.random_split(all_data , [300779, 75195])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): \n",
    "          super(Softmax, self).__init__()\n",
    "          self.linear1 = nn.Linear(2025,82,bias=True).to(device)\n",
    "    def forward(self, X): \n",
    "        X = self.linear1(X.view(X.shape[0],-1))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctPred(preds,truths):\n",
    "    correct = 0\n",
    "    for prediction,truth in zip(preds,truths):\n",
    "        pred = np.argmax(prediction)\n",
    "        if pred == truth:\n",
    "            correct = correct + 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): \n",
    "          super(MLP2, self).__init__()\n",
    "          self.linear1 = nn.Linear(2025,1000,bias=True).to(device)\n",
    "          self.linear2 = nn.Linear(1000,250,bias=True).to(device)\n",
    "          self.linear3 = nn.Linear(250,82,bias=True).to(device)\n",
    "     \n",
    "    def forward(self, X): \n",
    "        X = self.linear1(X.view(X.shape[0],-1))\n",
    "        X = F.relu(X)\n",
    "        X = self.linear2(X)\n",
    "        X = F.relu(X)\n",
    "        X = self.linear3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    '''Define your neural network'''\n",
    "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
    "    # TODO:\n",
    "    # You should create your neural network here\n",
    "    # TODO:\n",
    "      super(ConvNet, self).__init__()\n",
    "      self.conv1 = nn.Conv2d(1, 8, kernel_size=3,stride = 1).to(device)\n",
    "      self.conv2 = nn.Conv2d(8, 16, kernel_size=3,stride=1).to(device)\n",
    "      self.conv3 = nn.Conv2d(16, 32, kernel_size=3,stride=1).to(device)\n",
    "      self.mp = nn.MaxPool2d(2)\n",
    "      self.fc = nn.Linear(2592, 82).to(device) \n",
    "     \n",
    "    def forward(self, X): # you can add any additional parameters you want\n",
    "    # TODO:\n",
    "    # Forward propagation implementation should be here\n",
    "      length = X.shape[0]\n",
    "      X = self.conv1(X)\n",
    "      X = F.relu(X)\n",
    "      X = self.mp(self.conv2(X))\n",
    "      X = F.relu(X)\n",
    "      X = self.mp(self.conv3(X))\n",
    "      X = F.relu(X)\n",
    "      output = self.fc(X.view(X.size(0),-1))\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HINT: note that your training time should not take many days.\n",
    "\n",
    "#TODO:\n",
    "#Pick your hyper parameters\n",
    "max_epoch = 250\n",
    "train_batch = 50\n",
    "test_batch = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "def main(train_dataset,test_dataset): # you are free to change parameters\n",
    "    trainloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=train_batch,\n",
    "                                           shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=test_batch,\n",
    "                                           shuffle=False)\n",
    "    # initialize your GENet neural network\n",
    "    model = MLP2()\n",
    "    # define your loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9,weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    best_path = \"MLP3.pth\"\n",
    "    best_acc = 0\n",
    "    for epoch in range(max_epoch):\n",
    "      train(epoch, model, criterion, optimizer,trainloader,train_loss)\n",
    "      acc = test(model, criterion, test_loader,test_loss,test_acc)\n",
    "      if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model,best_path)\n",
    "    return model,train_loss,test_loss,test_acc\n",
    "''' Train your network for a one epoch '''\n",
    "def train(epoch, model, criterion, optimizer,loader,train_loss): # you are free to change parameters\n",
    "    model.train()\n",
    "    loss_total = []\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        # TODO:\n",
    "        # Implement training code for a one iteration\n",
    "        #data_time means data loading time per batch, and batch_time means forward/backward time during training.\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device).flatten()\n",
    "        data, labels= Variable(data), Variable(labels)#might be unnecessary\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total.append(loss.data)\n",
    "    \n",
    "    print(\"Epoch: \",epoch, \" loss: \",np.mean(np.array(loss_total)))\n",
    "    train_loss.append(np.mean(np.array(loss_total)))\n",
    "\n",
    "''' Test&Validate your network '''\n",
    "def test(model, criterion, loader,test_loss,test_acc): # you are free to change parameters\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            # TODO:\n",
    "            # Implement test code\n",
    "            count = count+1\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device).flatten()\n",
    "            data, labels= Variable(data), Variable(labels)\n",
    "            output = model(data)\n",
    "            total = total + output.shape[0]\n",
    "            correct = correct + correctPred(output,labels)\n",
    "            loss = criterion(output,labels)\n",
    "            total_loss = total_loss + loss.data\n",
    "    test_loss.append(total_loss/count)\n",
    "    test_acc.append(correct/total)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  loss:  2.7149465\n",
      "Epoch:  1  loss:  1.5265262\n",
      "Epoch:  2  loss:  1.1155239\n",
      "Epoch:  3  loss:  0.90034807\n",
      "Epoch:  4  loss:  0.7647346\n",
      "Epoch:  5  loss:  0.67289037\n",
      "Epoch:  6  loss:  0.6079046\n",
      "Epoch:  7  loss:  0.55604464\n",
      "Epoch:  8  loss:  0.5198705\n",
      "Epoch:  9  loss:  0.48412582\n",
      "Epoch:  10  loss:  0.45649776\n",
      "Epoch:  11  loss:  0.42933556\n",
      "Epoch:  12  loss:  0.4080041\n",
      "Epoch:  13  loss:  0.38822722\n",
      "Epoch:  14  loss:  0.37231737\n",
      "Epoch:  15  loss:  0.35539085\n",
      "Epoch:  16  loss:  0.341876\n",
      "Epoch:  17  loss:  0.32706535\n",
      "Epoch:  18  loss:  0.31730798\n",
      "Epoch:  19  loss:  0.30743992\n",
      "Epoch:  20  loss:  0.2948181\n",
      "Epoch:  21  loss:  0.28677672\n",
      "Epoch:  22  loss:  0.27742305\n",
      "Epoch:  23  loss:  0.2709831\n",
      "Epoch:  24  loss:  0.2613363\n",
      "Epoch:  25  loss:  0.25529188\n",
      "Epoch:  26  loss:  0.24956279\n",
      "Epoch:  27  loss:  0.24360178\n",
      "Epoch:  28  loss:  0.237392\n",
      "Epoch:  29  loss:  0.23224242\n",
      "Epoch:  30  loss:  0.22633481\n",
      "Epoch:  31  loss:  0.22176613\n",
      "Epoch:  32  loss:  0.21645635\n",
      "Epoch:  33  loss:  0.21241693\n",
      "Epoch:  34  loss:  0.20725003\n",
      "Epoch:  35  loss:  0.20401342\n",
      "Epoch:  36  loss:  0.2010056\n",
      "Epoch:  37  loss:  0.19731854\n",
      "Epoch:  38  loss:  0.19410056\n",
      "Epoch:  39  loss:  0.19062014\n",
      "Epoch:  40  loss:  0.18723626\n",
      "Epoch:  41  loss:  0.18439724\n",
      "Epoch:  42  loss:  0.1818426\n",
      "Epoch:  43  loss:  0.17818971\n",
      "Epoch:  44  loss:  0.17732547\n",
      "Epoch:  45  loss:  0.17504436\n",
      "Epoch:  46  loss:  0.17144719\n",
      "Epoch:  47  loss:  0.16975425\n",
      "Epoch:  48  loss:  0.16740367\n",
      "Epoch:  49  loss:  0.16651575\n",
      "Epoch:  50  loss:  0.16308226\n",
      "Epoch:  51  loss:  0.16125652\n",
      "Epoch:  52  loss:  0.16021606\n",
      "Epoch:  53  loss:  0.15830815\n",
      "Epoch:  54  loss:  0.15767525\n",
      "Epoch:  55  loss:  0.15589565\n",
      "Epoch:  56  loss:  0.15404582\n",
      "Epoch:  57  loss:  0.15163153\n",
      "Epoch:  58  loss:  0.15052854\n",
      "Epoch:  59  loss:  0.15053779\n",
      "Epoch:  60  loss:  0.14728573\n",
      "Epoch:  61  loss:  0.14761026\n",
      "Epoch:  62  loss:  0.14585568\n",
      "Epoch:  63  loss:  0.14587817\n",
      "Epoch:  64  loss:  0.14492904\n",
      "Epoch:  65  loss:  0.14358233\n",
      "Epoch:  66  loss:  0.14064558\n",
      "Epoch:  67  loss:  0.14168979\n",
      "Epoch:  68  loss:  0.14088537\n",
      "Epoch:  69  loss:  0.1393863\n",
      "Epoch:  70  loss:  0.13859709\n",
      "Epoch:  71  loss:  0.13718618\n",
      "Epoch:  72  loss:  0.13681388\n",
      "Epoch:  73  loss:  0.13611086\n",
      "Epoch:  74  loss:  0.13459916\n",
      "Epoch:  75  loss:  0.13502042\n",
      "Epoch:  76  loss:  0.13351849\n",
      "Epoch:  77  loss:  0.13298826\n",
      "Epoch:  78  loss:  0.13340285\n",
      "Epoch:  79  loss:  0.13209517\n",
      "Epoch:  80  loss:  0.13163611\n",
      "Epoch:  81  loss:  0.13072798\n",
      "Epoch:  82  loss:  0.12995057\n",
      "Epoch:  83  loss:  0.12963603\n",
      "Epoch:  84  loss:  0.12803116\n",
      "Epoch:  85  loss:  0.12751265\n",
      "Epoch:  86  loss:  0.1285575\n",
      "Epoch:  87  loss:  0.12681949\n",
      "Epoch:  88  loss:  0.12764406\n",
      "Epoch:  89  loss:  0.12651668\n",
      "Epoch:  90  loss:  0.12722923\n",
      "Epoch:  91  loss:  0.12551099\n",
      "Epoch:  92  loss:  0.12569259\n",
      "Epoch:  93  loss:  0.12500827\n",
      "Epoch:  94  loss:  0.12366141\n",
      "Epoch:  95  loss:  0.123652235\n",
      "Epoch:  96  loss:  0.122346655\n",
      "Epoch:  97  loss:  0.12336185\n",
      "Epoch:  98  loss:  0.12265875\n",
      "Epoch:  99  loss:  0.12224697\n",
      "Epoch:  100  loss:  0.122312516\n",
      "Epoch:  101  loss:  0.122134104\n",
      "Epoch:  102  loss:  0.12142673\n",
      "Epoch:  103  loss:  0.12008937\n",
      "Epoch:  104  loss:  0.11993469\n",
      "Epoch:  105  loss:  0.12112673\n",
      "Epoch:  106  loss:  0.120072216\n",
      "Epoch:  107  loss:  0.11939859\n",
      "Epoch:  108  loss:  0.11821901\n",
      "Epoch:  109  loss:  0.11840581\n",
      "Epoch:  110  loss:  0.11843594\n",
      "Epoch:  111  loss:  0.11865257\n",
      "Epoch:  112  loss:  0.11884958\n",
      "Epoch:  113  loss:  0.11957392\n",
      "Epoch:  114  loss:  0.118381836\n",
      "Epoch:  115  loss:  0.118926555\n",
      "Epoch:  116  loss:  0.11792244\n",
      "Epoch:  117  loss:  0.1169267\n",
      "Epoch:  118  loss:  0.11625111\n"
     ]
    }
   ],
   "source": [
    "trained_model,train_loss,test_loss,test_acc = main(train_dataset,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)),train_loss , label='Training loss')\n",
    "plt.plot(range(len(test_loss)),test_loss , label='Test loss')\n",
    "plt.xlabel('Epoch ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch MLP3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(test_acc)),test_acc,label='Test accuracy')\n",
    "plt.xlabel('Epoch ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Epoch MLP2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(np.reshape(x,(375974,-1)),y)\n",
    "clf = svm.SVC(kernel='linear', C=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "predict = clf.predict(X_test)\n",
    "correct = correctPred(predict,y_test)\n",
    "print(\"Accuracy: \" , correct/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375974, 1, 45, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
